{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "\n",
    "#import os ; os.environ['KERAS_BACKEND']='tensorflow'\n",
    "\n",
    "import keras\n",
    "import keras.backend as K\n",
    "\n",
    "import numpy as np\n",
    "import scipy.optimize\n",
    "import scipy.misc\n",
    "from collections import OrderedDict, namedtuple\n",
    "\n",
    "if K._BACKEND == 'tensorflow':\n",
    "    import tensorflow as tf\n",
    "    def tensor_eye(size):\n",
    "        return tf.eye(size)\n",
    "elif K._BACKEND == 'theano':\n",
    "    import theano.tensor as T\n",
    "    def tensor_eye(size):\n",
    "        return T.eye(size)\n",
    "else:\n",
    "    raise Exception('Unknown backend')\n",
    "\n",
    "    \n",
    "def get_mnist():\n",
    "    nb_classes = 10\n",
    "    (X_train, y_train), (X_test, y_test) = keras.datasets.mnist.load_data()\n",
    "    X_train = np.reshape(X_train, [X_train.shape[0], -1]).astype('float32') / 255.\n",
    "    X_test  = np.reshape(X_test , [X_test.shape[0] , -1]).astype('float32') / 255.\n",
    "    X_train = X_train * 2.0 - 1.0\n",
    "    X_test  = X_test  * 2.0 - 1.0\n",
    "\n",
    "    Y_train = keras.utils.np_utils.to_categorical(y_train, nb_classes)\n",
    "    Y_test  = keras.utils.np_utils.to_categorical(y_test, nb_classes)\n",
    "\n",
    "    Dataset = namedtuple('Dataset',['X','Y','y','nb_classes'])\n",
    "    trn = Dataset(X_train, Y_train, y_train, nb_classes)\n",
    "    tst = Dataset(X_test , Y_test, y_test, nb_classes)\n",
    "\n",
    "    del X_train, X_test, Y_train, Y_test, y_train, y_test\n",
    "    \n",
    "    return trn, tst\n",
    "\n",
    "trn, tst = get_mnist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sgd_batchsize = 128\n",
    "\n",
    "def Kget_dists(X):\n",
    "    \"\"\"Keras code to compute the pairwise distance matrix for a set of\n",
    "    vectors specifie by the matrix X.\n",
    "    \"\"\"\n",
    "    x2 = K.expand_dims(K.sum(K.square(X), axis=1), 1)\n",
    "    dists = x2 + K.transpose(x2) - 2*K.dot(X, K.transpose(X))\n",
    "    return dists\n",
    "    \n",
    "def kde_entropy_from_dists_loo(dists, x, var, exponent=1.0):\n",
    "    # exponent=1.0 for KL estimator, 0.25 for BD\n",
    "    dims = K.cast( K.shape(x)[1], K.floatx() ) \n",
    "    N    = K.cast( K.shape(x)[0], K.floatx() )\n",
    "    \n",
    "    dists2 = dists + tensor_eye(K.cast(N, 'int32')) * 10e20\n",
    "    dists2 = dists2 / (2*var)\n",
    "    normconst = (dims/2.0)*K.log(2*np.pi*var)\n",
    "    lprobs  = K.logsumexp(-dists2, axis=1) - K.log(N-1) - normconst\n",
    "    h = -K.mean(lprobs)\n",
    "    return h\n",
    "\n",
    "def entropy_estimator(x, var, exponent=1.0):\n",
    "    # exponent=1.0 for KL estimator, 0.25 for BD\n",
    "    dims = K.cast( K.shape(x)[1], K.floatx() ) \n",
    "    N    = K.cast( K.shape(x)[0], K.floatx() )\n",
    "    dists = Kget_dists(x)\n",
    "    dists2 = dists / (2*var)\n",
    "    normconst = (dims/2.0)*K.log(2*np.pi*var)\n",
    "    lprobs = K.logsumexp(-exponent * dists2 - normconst, axis=1) - K.log(N)\n",
    "    h = -K.mean(lprobs)\n",
    "    return dims/2 + h\n",
    "\n",
    "def entropy_estimator2(x, var):\n",
    "    # exponent=1.0 for KL estimator, 0.25 for BD\n",
    "    dims = K.cast( K.shape(x)[1], K.floatx() ) \n",
    "    N    = K.cast( K.shape(x)[0], K.floatx() )\n",
    "    dists = Kget_dists(x)\n",
    "    dists2 = dists / (2*var)\n",
    "    normconst = (dims/2.0)*K.log(2*np.pi*var)\n",
    "    lprobs = K.logsumexp(-dists2, axis=1) - K.log(N) - normconst\n",
    "    h = -K.mean(lprobs)\n",
    "    return dims/2 + h\n",
    "\n",
    "def wrapKfunc(f, data, targets):\n",
    "    def callf(logvar):\n",
    "        return f([data, targets, np.ones(len(data)),1,np.exp(logvar)])[0].flat[0]\n",
    "    return callf\n",
    "\n",
    "class Reporter(keras.callbacks.Callback):\n",
    "    def __init__(self, on_every=1, *kargs, **kwargs):\n",
    "        super(Reporter, self).__init__(*kargs, **kwargs)\n",
    "        self.on_every = on_every\n",
    "        \n",
    "    def on_train_begin(self, logs={}):\n",
    "        self.layerfuncs = []\n",
    "        var = K.placeholder(ndim=0)\n",
    "        inputs = self.model.inputs + self.model.targets + self.model.sample_weights + [ K.learning_phase(),] + [var,]\n",
    "        for lndx, l in enumerate(self.model.layers[2:-1]):\n",
    "            f = K.function(inputs, [kde_entropy_from_dists_loo(Kget_dists(l.output),l.output,var)])\n",
    "            f2 = K.function(inputs, [entropy_estimator(l.output,var)])\n",
    "            f3 = K.function(inputs, [entropy_estimator(l.output,var, 0.25)])\n",
    "            self.layerfuncs.append([f, f2, f3])\n",
    "        self.saved_logs = {}\n",
    "            \n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "        if not (epoch % self.on_every == 0):\n",
    "            return\n",
    "            \n",
    "        l = OrderedDict()\n",
    "        for lndx, [f_entropy_loo, f_entropy_upper, f_entropy_lower] in enumerate(self.layerfuncs):\n",
    "            #def callf(var):\n",
    "            #    return self.Kfunc([trn.X[::20], trn.Y[::20], np.ones(len(trn.X[::20])),1,np.exp(logvar)])[0].flat[0]\n",
    "            #   \n",
    "            r = scipy.optimize.minimize_scalar(wrapKfunc(f_entropy_loo, trn.X[::20], trn.Y[::20]), method='brent')\n",
    "            l['trn_layer_%d_h_loo'%lndx] = r.fun\n",
    "            l['trn_layer_%d_logvar'%lndx] = r.x\n",
    "            l['trn_layer_%d_h_upper'%lndx] = wrapKfunc(f_entropy_upper, trn.X[::20], trn.Y[::20])(r.x)\n",
    "            l['trn_layer_%d_h_lower'%lndx] = wrapKfunc(f_entropy_lower, trn.X[::20], trn.Y[::20])(r.x)\n",
    "\n",
    "            r = scipy.optimize.minimize_scalar(wrapKfunc(f_entropy_loo, tst.X[::10], tst.Y[::10]), method='brent')\n",
    "            l['tst_layer_%d_h_loo' %lndx] = r.fun\n",
    "            l['tst_layer_%d_logvar'%lndx] = r.x\n",
    "            l['tst_layer_%d_h_upper'%lndx] = wrapKfunc(f_entropy_upper, tst.X[::10], tst.Y[::10])(r.x)\n",
    "            l['tst_layer_%d_h_lower'%lndx] = wrapKfunc(f_entropy_lower, tst.X[::10], tst.Y[::10])(r.x)\n",
    "            \n",
    "            \n",
    "        for k,v in l.items():\n",
    "            print(k,\"=\",v)\n",
    "            logs[k] = v\n",
    "            \n",
    "        self.saved_logs[epoch] = l.copy()\n",
    "        \n",
    "            \n",
    "input_layer  = keras.layers.Input((trn.X.shape[1],))\n",
    "hidden_output = keras.layers.Dense(1024, activation='relu')(input_layer)\n",
    "hidden_output = keras.layers.Dense(20, activation='relu')(hidden_output)\n",
    "hidden_output = keras.layers.Dense(20 , activation='relu')(hidden_output)\n",
    "\n",
    "outputs  = keras.layers.Dense(trn.nb_classes, activation='softmax')(hidden_output)\n",
    "model = keras.models.Model(inputs=input_layer, outputs=outputs)\n",
    "optimizer = keras.optimizers.SGD(lr=0.001)# , momentum=0.9, nesterov=True)\n",
    "model.compile(loss='categorical_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n",
    "\n",
    "reporter   = Reporter(on_every=10)\n",
    "#lrcb       = keras.callbacks.LearningRateScheduler(lrscheduler)\n",
    "\n",
    "r = model.fit(x=trn.X, y=trn.Y, verbose=2, batch_size=sgd_batchsize, epochs=20000, \n",
    "              validation_data=(tst.X, tst.Y), callbacks=[reporter,])\n",
    "              #callbacks=[lrcb,])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "plt.figure(figsize=(15,10))\n",
    "for rowndx, t in enumerate(['trn','tst']):\n",
    "    plt.subplot(2,2,rowndx*2+1)\n",
    "    epochs = sorted(reporter.saved_logs.keys())\n",
    "    \n",
    "    plt.plot(epochs, [reporter.saved_logs[epoch][t+'_layer_0_h_upper'] for epoch in epochs], 'r', label=\"%s $H_{KL}$(layer1)\"%t)\n",
    "    plt.plot(epochs, [reporter.saved_logs[epoch][t+'_layer_0_h_lower'] for epoch in epochs], 'r--', label=\"%s $H_{BD}$(layer1)\"%t)\n",
    "    plt.plot(epochs, [reporter.saved_logs[epoch][t+'_layer_1_h_upper'] for epoch in epochs], 'b', label=\"%s $H_{KL}$(layer2)\"%t)\n",
    "    plt.plot(epochs, [reporter.saved_logs[epoch][t+'_layer_1_h_lower'] for epoch in epochs], 'b--', label=\"%s $H_{BD}$(layer2)\"%t)\n",
    "    plt.legend(loc='lower right')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Layer Activity Entropy')\n",
    "    #plt.savefig('run2.pdf')\n",
    "\n",
    "    plt.subplot(2,2,rowndx*2+2)\n",
    "    plt.plot(epochs, [reporter.saved_logs[epoch][t+'_layer_0_h_loo'] for epoch in epochs], 'r', label=\"%s $H_{loo}$(layer1)\"%t)\n",
    "    plt.plot(epochs, [reporter.saved_logs[epoch][t+'_layer_1_h_loo'] for epoch in epochs], 'b', label=\"%s $H_{loo}$(layer2)\"%t)\n",
    "\n",
    "    plt.legend(loc='lower right')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Layer Activity Entropy')\n",
    "    \n",
    "plt.savefig('run_output.pdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "multivariate_normal([0.5, -0.2], [[2.0, 0.3], [0.3, 0.5]])\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
