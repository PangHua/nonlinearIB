{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "\n",
    "#import os ; os.environ['KERAS_BACKEND']='cntk'\n",
    "\n",
    "import keras\n",
    "import keras.backend as K\n",
    "\n",
    "import numpy as np\n",
    "import scipy.optimize\n",
    "import scipy.misc\n",
    "from collections import OrderedDict, namedtuple\n",
    "\n",
    "transpose = K.transpose\n",
    "if K._BACKEND == 'tensorflow':\n",
    "    import tensorflow as tf\n",
    "    def tensor_eye(size):\n",
    "        return tf.eye(size)\n",
    "elif K._BACKEND == 'theano':\n",
    "    import theano.tensor as T\n",
    "    def tensor_eye(size):\n",
    "        return T.eye(size)\n",
    "elif K._BACKEND == 'cntk':\n",
    "    raise Exception('Unsupported')\n",
    "    import cntk as C\n",
    "    def transpose(x):\n",
    "        return C.swapaxes(x, -1, -2)\n",
    "else:\n",
    "    raise Exception('Unknown backend')\n",
    "\n",
    "    \n",
    "def get_mnist():\n",
    "    nb_classes = 10\n",
    "    (X_train, y_train), (X_test, y_test) = keras.datasets.mnist.load_data()\n",
    "    X_train = np.reshape(X_train, [X_train.shape[0], -1]).astype('float32') / 255.\n",
    "    X_test  = np.reshape(X_test , [X_test.shape[0] , -1]).astype('float32') / 255.\n",
    "    X_train = X_train * 2.0 - 1.0\n",
    "    X_test  = X_test  * 2.0 - 1.0\n",
    "\n",
    "    Y_train = keras.utils.np_utils.to_categorical(y_train, nb_classes).astype('float32')\n",
    "    Y_test  = keras.utils.np_utils.to_categorical(y_test, nb_classes).astype('float32')\n",
    "\n",
    "    Dataset = namedtuple('Dataset',['X','Y','y','nb_classes'])\n",
    "    trn = Dataset(X_train, Y_train, y_train, nb_classes)\n",
    "    tst = Dataset(X_test , Y_test, y_test, nb_classes)\n",
    "\n",
    "    del X_train, X_test, Y_train, Y_test, y_train, y_test\n",
    "    \n",
    "    return trn, tst\n",
    "\n",
    "trn, tst = get_mnist()\n",
    "PLOT_LAYERS    = [2,3]\n",
    "NUM_EPOCHS     = 10000\n",
    "NUMBER_OF_BINS = 3\n",
    "\n",
    "SGD_BATCHSIZE = 128\n",
    "\n",
    "ACTIVATION = 'relu'\n",
    "ACTIVATION = 'tanh'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Simplified MI computation code from https://github.com/ravidziv/IDNNs\n",
    "\n",
    "def get_unique_probs(x):\n",
    "    uniqueids = np.ascontiguousarray(x).view(np.dtype((np.void, x.dtype.itemsize * x.shape[1])))\n",
    "    _, unique_inverse, unique_counts = np.unique(uniqueids, return_index=False, return_inverse=True, return_counts=True)\n",
    "    return np.asarray(unique_counts / float(sum(unique_counts))), unique_inverse\n",
    "\n",
    "def bin_calc_information(inputdata, layerdata, num_of_bins):\n",
    "    p_xs, unique_inverse_x = get_unique_probs(inputdata)\n",
    "    \n",
    "    bins = np.linspace(-1, 1, num_of_bins, dtype='float32') \n",
    "    digitized = bins[np.digitize(np.squeeze(layerdata.reshape(1, -1)), bins) - 1].reshape(len(layerdata), -1)\n",
    "    p_ts, _ = get_unique_probs( digitized )\n",
    "    \n",
    "    H2 = -np.sum(p_ts * np.log(p_ts))\n",
    "    H2X = 0.\n",
    "    for xval in range(len(p_xs)):\n",
    "        p_t_given_x, _ = get_unique_probs(digitized[unique_inverse_x == xval, :])\n",
    "        H2X += - p_xs[xval] * np.sum(p_t_given_x * np.log(p_t_given_x))\n",
    "    return H2 - H2X\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "def Kget_dists(X):\n",
    "    \"\"\"Keras code to compute the pairwise distance matrix for a set of\n",
    "    vectors specifie by the matrix X.\n",
    "    \"\"\"\n",
    "    x2 = K.expand_dims(K.sum(K.square(X), axis=1), 1)\n",
    "    dists = x2 + transpose(x2) - 2*K.dot(X, transpose(X))\n",
    "    return dists\n",
    "\n",
    "def get_shape(x):\n",
    "    dims = K.cast( K.shape(x)[1], K.floatx() ) \n",
    "    N    = K.cast( K.shape(x)[0], K.floatx() )\n",
    "    return dims, N\n",
    "\n",
    "def kde_entropy_from_dists_loo(dists, x, var):\n",
    "    dims, N = get_shape(x)\n",
    "    dists2 = dists + tensor_eye(K.cast(N, 'int32')) * 10e20\n",
    "    dists2 = dists2 / (2*var)\n",
    "    normconst = (dims/2.0)*K.log(2*np.pi*var)\n",
    "    lprobs  = K.logsumexp(-dists2, axis=1) - K.log(N-1) - normconst\n",
    "    h = -K.mean(lprobs)\n",
    "    return h\n",
    "\n",
    "def entropy_estimator(x, var):\n",
    "    dims, N = get_shape(x)\n",
    "    dists = Kget_dists(x)\n",
    "    dists2 = dists / (2*var)\n",
    "    normconst = (dims/2.0)*K.log(2*np.pi*var)\n",
    "    lprobs = K.logsumexp(-dists2, axis=1) - K.log(N) - normconst\n",
    "    h = -K.mean(lprobs)\n",
    "    return dims/2 + h\n",
    "\n",
    "def wrapKfunc(f, data):\n",
    "    def callf(logvar):\n",
    "        return f([data, 1, np.exp(logvar)])[0].flat[0]\n",
    "    return callf\n",
    "\n",
    "class Reporter(keras.callbacks.Callback):\n",
    "    def __init__(self, on_every=1, *kargs, **kwargs):\n",
    "        super(Reporter, self).__init__(*kargs, **kwargs)\n",
    "        self.on_every = on_every\n",
    "        \n",
    "    def on_train_begin(self, logs={}):\n",
    "        self.layerfuncs = []\n",
    "        var = K.placeholder(ndim=0)\n",
    "        inputs = self.model.inputs + [ K.learning_phase(),] + [var,]\n",
    "        for lndx in PLOT_LAYERS:\n",
    "            l = self.model.layers[lndx]\n",
    "            cfuncs = {}\n",
    "            cfuncs['loo']    = K.function(inputs, [kde_entropy_from_dists_loo(Kget_dists(l.output),l.output,var)])\n",
    "            cfuncs['upper']  = K.function(inputs, [entropy_estimator(l.output,var)])\n",
    "            #cfuncs['lower']  = K.function(inputs, [entropy_estimator(l.output,var, 0.25)])\n",
    "            cfuncs['lower'] = K.function(inputs, [entropy_estimator(l.output,4*var)+np.log(0.25)*l.output.shape[1]/2.])\n",
    "            #cfuncs['lower']  = K.function(inputs, [entropy_estimator(l.output,var, 0.25)])\n",
    "            cfuncs['output'] = K.function(self.model.inputs + [ K.learning_phase(),], [l.output])\n",
    "            self.layerfuncs.append(cfuncs)\n",
    "        self.saved_logs = {}\n",
    "            \n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "        if not (epoch % self.on_every == 0):\n",
    "            return\n",
    "            \n",
    "        l = OrderedDict()\n",
    "        for lndx, cfuncs in enumerate(self.layerfuncs):\n",
    "            trndata = trn.X[::20]\n",
    "            tstdata = tst.X[::10]\n",
    "            r = scipy.optimize.minimize_scalar(wrapKfunc(cfuncs['loo'], trndata), method='brent')\n",
    "            l['trn_layer_%d_h_loo'%lndx] = r.fun\n",
    "            l['trn_layer_%d_logvar'%lndx] = r.x\n",
    "            l['trn_layer_%d_h_upper'%lndx] = wrapKfunc(cfuncs['upper'], trndata)(r.x)\n",
    "            l['trn_layer_%d_h_lower'%lndx] = wrapKfunc(cfuncs['lower'], trndata)(r.x)\n",
    "\n",
    "            r = scipy.optimize.minimize_scalar(wrapKfunc(cfuncs['loo'], tstdata), method='brent')\n",
    "            l['tst_layer_%d_h_loo' %lndx] = r.fun\n",
    "            l['tst_layer_%d_logvar'%lndx] = r.x\n",
    "            l['tst_layer_%d_h_upper'%lndx] = wrapKfunc(cfuncs['upper'], tstdata)(r.x)\n",
    "            l['tst_layer_%d_h_lower'%lndx] = wrapKfunc(cfuncs['lower'], tstdata)(r.x)\n",
    "            \n",
    "            trndata = trn.X[::10]\n",
    "            tstdata = tst.X\n",
    "            trnlayeroutput= cfuncs['output']([trndata, 0])[0]\n",
    "            tstlayeroutput= cfuncs['output']([tstdata, 0])[0]\n",
    "            l['trn_layer_%d_h_bin'%lndx] = bin_calc_information(trndata, trnlayeroutput,num_of_bins=NUMBER_OF_BINS)\n",
    "            l['tst_layer_%d_h_bin'%lndx] = bin_calc_information(tstdata, tstlayeroutput,num_of_bins=NUMBER_OF_BINS)\n",
    "            l['trn_layer_%d_h_binstd'%lndx] = bin_calc_information(trndata,0.5* scipy.stats.zscore(trnlayeroutput),num_of_bins=NUMBER_OF_BINS)\n",
    "            l['tst_layer_%d_h_binstd'%lndx] = bin_calc_information(tstdata,0.5* scipy.stats.zscore(tstlayeroutput),num_of_bins=NUMBER_OF_BINS)\n",
    "\n",
    "            \n",
    "        for k,v in l.items():\n",
    "            print(k,\"=\",v)\n",
    "            logs[k] = v\n",
    "            \n",
    "        self.saved_logs[epoch] = l.copy()\n",
    "        \n",
    "            \n",
    "input_layer  = keras.layers.Input((trn.X.shape[1],))\n",
    "hidden_output = keras.layers.Dense(1024, activation=ACTIVATION)(input_layer)\n",
    "hidden_output = keras.layers.Dense(20  , activation=ACTIVATION)(hidden_output)\n",
    "hidden_output = keras.layers.Dense(20  , activation=ACTIVATION)(hidden_output)\n",
    "\n",
    "outputs  = keras.layers.Dense(trn.nb_classes, activation='softmax')(hidden_output)\n",
    "model = keras.models.Model(inputs=input_layer, outputs=outputs)\n",
    "optimizer = keras.optimizers.SGD(lr=0.001)\n",
    "model.compile(loss='categorical_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n",
    "\n",
    "reporter   = Reporter(on_every=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "r = model.fit(x=trn.X, y=trn.Y, verbose=2, batch_size=SGD_BATCHSIZE, epochs=NUM_EPOCHS, \n",
    "              validation_data=(tst.X, tst.Y), callbacks=[reporter,])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "plt.figure(figsize=(15,5*len(PLOT_LAYERS)))\n",
    "for rowndx, lndx in enumerate(PLOT_LAYERS):\n",
    "    for colndx, t in enumerate(['trn','tst']):\n",
    "        plt.subplot(len(PLOT_LAYERS),2,rowndx*len(PLOT_LAYERS)+colndx+1)\n",
    "        epochs = sorted(reporter.saved_logs.keys())\n",
    "\n",
    "        plt.plot(epochs, [reporter.saved_logs[epoch][t+'_layer_%d_h_upper' % rowndx] for epoch in epochs], 'r:', label=\"$H_{KL}$\")\n",
    "        plt.plot(epochs, [reporter.saved_logs[epoch][t+'_layer_%d_h_lower' % rowndx] for epoch in epochs], 'g:', label=\"$H_{BD}$\")\n",
    "        \n",
    "        plt.plot(epochs, [reporter.saved_logs[epoch][t+'_layer_%d_h_loo'   % rowndx] for epoch in epochs], 'r', label=\"$H_{loo}$\")\n",
    "        plt.plot(epochs, [reporter.saved_logs[epoch][t+'_layer_%d_h_bin'   % rowndx] for epoch in epochs], 'b', label=\"$H_{bin}$\")\n",
    "        plt.plot(epochs, [reporter.saved_logs[epoch][t+'_layer_%d_h_binstd'% rowndx] for epoch in epochs], 'k', label=\"$H_{binstd}$\")\n",
    "\n",
    "        plt.title('%s layer %d'%(t,lndx))\n",
    "        plt.xlabel('Epochs')\n",
    "        plt.ylabel('Layer Activity Entropy')\n",
    "        \n",
    "        if rowndx ==1 and t == 'tst':\n",
    "            plt.legend(loc='lower right')\n",
    "    \n",
    "plt.savefig('run_output_tanh.pdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Original MI computation code from https://github.com/ravidziv/IDNNs\n",
    "\n",
    "def extract_probs(label, x):\n",
    "    \"\"\"calculate the probabilities of the given data and labels p(x), p(y) and (y|x)\"\"\"\n",
    "    pys = np.sum(label, axis=0) / float(label.shape[0])\n",
    "    b = np.ascontiguousarray(x).view(np.dtype((np.void, x.dtype.itemsize * x.shape[1])))\n",
    "    unique_array, unique_indices, unique_inverse_x, unique_counts = \\\n",
    "        np.unique(b, return_index=True, return_inverse=True, return_counts=True)\n",
    "    unique_a = x[unique_indices]\n",
    "    b1 = np.ascontiguousarray(unique_a).view(np.dtype((np.void, unique_a.dtype.itemsize * unique_a.shape[1])))\n",
    "    pxs = unique_counts / float(np.sum(unique_counts))\n",
    "    p_y_given_x = []\n",
    "    for i in range(0, len(unique_array)):\n",
    "        indexs = unique_inverse_x == i\n",
    "        py_x_current = np.mean(label[indexs, :], axis=0)\n",
    "        p_y_given_x.append(py_x_current)\n",
    "    p_y_given_x = np.array(p_y_given_x).T\n",
    "    b_y = np.ascontiguousarray(label).view(np.dtype((np.void, label.dtype.itemsize * label.shape[1])))\n",
    "    unique_array_y, unique_indices_y, unique_inverse_y, unique_counts_y = \\\n",
    "        np.unique(b_y, return_index=True, return_inverse=True, return_counts=True)\n",
    "    pys1 = unique_counts_y / float(np.sum(unique_counts_y))\n",
    "    return pys, pys1, p_y_given_x, b, unique_a, unique_inverse_x, unique_inverse_y, pxs\n",
    "\n",
    "def calc_information_sampling(data, bins, pys1, pxs, label, b, p_YgX, unique_inverse_x,  unique_inverse_y):\n",
    "    bins = bins.astype(np.float32)\n",
    "    num_of_bins = bins.shape[0]\n",
    "    # bins = stats.mstats.mquantiles(np.squeeze(data.reshape(1, -1)), np.linspace(0,1, num=num_of_bins))\n",
    "    # hist, bin_edges = np.histogram(np.squeeze(data.reshape(1, -1)), normed=True)\n",
    "    digitized = bins[np.digitize(np.squeeze(data.reshape(1, -1)), bins) - 1].reshape(len(data), -1)\n",
    "    b2 = np.ascontiguousarray(digitized).view(\n",
    "        np.dtype((np.void, digitized.dtype.itemsize * digitized.shape[1])))\n",
    "    unique_array, unique_inverse_t, unique_counts = \\\n",
    "        np.unique(b2, return_index=False, return_inverse=True, return_counts=True)\n",
    "    p_ts = unique_counts / float(sum(unique_counts))\n",
    "    PXs, PYs = np.asarray(pxs).T, np.asarray(pys1).T\n",
    "    local_IXT, local_ITY = calc_information_from_mat(PXs, PYs, p_ts, digitized, unique_inverse_x, unique_inverse_y,\n",
    "                                                     unique_array)\n",
    "    return local_IXT, local_ITY\n",
    "\n",
    "def calc_condtion_entropy(px, t_data, unique_inverse_x):\n",
    "    # Condition entropy of t given x\n",
    "    H2X_array = np.array(list(\n",
    "        calc_entropy_for_specipic_t(t_data[unique_inverse_x == i, :], px[i])\n",
    "                                   for i in range(px.shape[0])))\n",
    "    H2X = np.sum(H2X_array)\n",
    "    return H2X\n",
    "\n",
    "\n",
    "def calc_information_from_mat(px, py, ps2, data, unique_inverse_x, unique_inverse_y, unique_array):\n",
    "    \"\"\"Calculate the MI based on binning of the data\"\"\"\n",
    "    H2 = -np.sum(ps2 * np.log(ps2))\n",
    "    H2X = calc_condtion_entropy(px, data, unique_inverse_x)\n",
    "    H2Y = calc_condtion_entropy(py.T, data, unique_inverse_y)\n",
    "    IY = H2 - H2Y\n",
    "    IX = H2 - H2X\n",
    "    return IX, IY\n",
    "\n",
    "def calc_entropy_for_specipic_t(current_ts, px_i):\n",
    "    \"\"\"Calc entropy for specipic t\"\"\"\n",
    "    b2 = np.ascontiguousarray(current_ts).view(\n",
    "        np.dtype((np.void, current_ts.dtype.itemsize * current_ts.shape[1])))\n",
    "    unique_array, unique_inverse_t, unique_counts = \\\n",
    "        np.unique(b2, return_index=False, return_inverse=True, return_counts=True)\n",
    "    p_current_ts = unique_counts / float(sum(unique_counts))\n",
    "    p_current_ts = np.asarray(p_current_ts, dtype=np.float64).T\n",
    "    H2X = px_i * (-np.sum(p_current_ts * np.log(p_current_ts)))\n",
    "    return H2X\n",
    "\n",
    "if False:\n",
    "    inputs = model.inputs + [ K.learning_phase(),] \n",
    "    num_of_bins = 10\n",
    "    bins = np.linspace(-1, 1, num_of_bins)\n",
    "\n",
    "    for lndx, l in enumerate(model.layers[2:-1]):\n",
    "        inputdata = trn.X[::20]\n",
    "        label = trn.Y[::20]\n",
    "        data = K.function(inputs, [l.output])([inputdata, 0])[0]\n",
    "        pys, pys1, p_YgX, b, unique_a, unique_inverse_x, unique_inverse_y, pxs = extract_probs(label, inputdata)\n",
    "\n",
    "        local_IXT, local_ITY = calc_information_sampling(data, bins, pys1, pxs, label, b, p_YgX, unique_inverse_x,\n",
    "                                  unique_inverse_y)\n",
    "\n",
    "        print(local_IXT, local_ITY)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
